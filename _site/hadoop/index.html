<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Distributed Data Mining Lab Course (TUM)</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Course materials and notes for TUM Lab Course: Distributed Data Mining.">
    <link rel="canonical" href="/hadoop/">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

    <!-- Google fonts -->
    <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>

    <!-- Google tracking -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-46895817-2', 'auto');
      ga('send', 'pageview');

    </script>
    
</head>


    <body>

    <header class="site-header">

  <div class="wrap title-wrap">
    <a class="site-title" href="/">Distributed Data Mining Lab Course (TUM)</a>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1></h1>
  </header>

  <article class="post-content">
  <p>Table of Contents:</p>

<ul>
<li><a href="#installation">Installation</a></li>
<li><a href="#standalone">Standalone MapReduce</a></li>
<li><a href="#onenode">Setup a one-node-cluster</a></li>
<li><a href="#multinode">Setup a multi-node-cluster</a></li>
<li><a href="#upload">Using HDFS</a></li>
</ul>

<p><a name='installation'></a></p>

<h3>Installation</h3>

<ol>
<li>Start an ubuntu instance and set up <code>/etc/resolv.conf</code> and <code>/etc/hosts</code> </li>
<li>Update apt-get repository : <code>sudo apt-get update</code></li>
<li>Install java 1.7 with <code>sudo aptitude -y install openjdk-7-jdk</code></li>
<li>Edit your <code>.bashrc file</code> and add <code>export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64</code></li>
<li>Get the newest version of hadoop with <code>sudo wget http://apache.openmirror.de/hadoop/common/hadoop-2.5.2/hadoop-2.5.2.tar.gz</code></li>
<li>Unpack hadoop with <code>sudo tar â€“xvzf hadoop-2.5.2.tar.gz</code></li>
<li>Rename the folder with <code>sudo mv hadoop-2.5.2 hadoop</code></li>
<li>Adjust the <code>JAVA_HOME</code> variable in <code>etc/hadoop/hadoop-env.sh</code></li>
</ol>

<p><a name='standalone'></a></p>

<h3>Standalone MapReduce</h3>

<p>By default, Hadoop is configured to run in non-distributed mode. You can start a small mapreduce job by following:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">$ mkdir input
$ cp etc/hadoop/*.xml input
$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar 
$ grep input output &#39;dfs[a-z.]+&#39;
$ cat output/* //which should deliver 1 dfsadmin
</code></pre></div>
<p>Congratulations! You have Hadoop running on Ubuntu.</p>

<p><a name='onenode'></a></p>

<h3>Setup a one-node-cluster</h3>

<p>Hadoop can also be run on a single-node in a pseudo-distributed mode where each Hadoop daemon runs in a separate Java process.
First of all we need to shut of IPv6 because Ubuntu is using 0.0.0.0 for different Hadoop instances
Edit file <code>/etc/sysctl.conf</code> with below and reboot or just add <code>export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true</code> in <code>hadoop-env</code></p>
<div class="highlight"><pre><code class="language-text" data-lang="text">#disable ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
</code></pre></div>
<p>Edit file etc/hadoop/core-site.xml:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">&lt;configuration&gt;
&lt;property&gt;
&lt;name&gt;fs.defaultFS&lt;/name&gt;
&lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre></div>
<p>Edit file etc/hadoop/hdfs-site.xml</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">&lt;configuration&gt;
&lt;property&gt;
&lt;name&gt;dfs.replication&lt;/name&gt;
&lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre></div>
<p>Hadoop uses ssh to communicate so we need to setup ssh localhost
Lets create a key: <code>ssh-keygen -t dsa -f ~/.ssh/id_dsa</code>
And add this key <code>cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</code>, now it should be possible to ssh localhost</p>

<p><a name='multinode'></a></p>

<h3>Setup a multi-node-cluster</h3>

<p>This Tutorial is an excerpt of Mohamed and Marawan&#39;s Hadoop Tutorial.
The <a href="http://hadoop.apache.org/docs/r2.5.1/hadoop-project-dist/hadoop-common/ClusterSetup.html">Hadoop tutorial for multi-node cluster installation</a> doesn&#39;t offer much in the way of detailed steps, presenting just a list of the configuration options available, which is confusing because for a minimal required configuration you only need to change the following :
In <code>etc/hadoop/core-site.xml</code> , instead of having the <code>fs.defaultFS</code> property set to localhost, we set it to point to the IP address of the NameNode :</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"> &lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://172.16.19.10:9000&lt;/value&gt;
    &lt;/property&gt;
 &lt;/configuration&gt;
</code></pre></div>
<p>This has 2 effects. First on the NameNode, the NameNode process will listen on 172.16.19.10:9000 which allows machines on the network to connect to it, in contrast with listening on localhost:9000 which only allowed processed on the same machine to connect to it. The second effect is on the DataNode, this config of course just tells the DataNodes what NameNode to connect to and on which port.</p>

<p>In <code>etc/hadoop/yarn-site.xml</code>, specify the ResourceManager IP, which here we just used the NameNode IP for (but that doesn&#39;t always have to be the case) :</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"> &lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
        &lt;value&gt;172.16.19.10&lt;/value&gt;
        &lt;description&gt;The hostname of the Resource Manager.&lt;/description&gt;
    &lt;/property&gt;
 &lt;/configuration&gt;
</code></pre></div>
<p>The above configs need to eventually be on both the NameNode and the DataNode, so we applied them to our single machine , then created a snapshot of the machine. And that is our Hadoop image that we&#39;ll use to create our cluster.</p>

<p>(Optional) In <code>etc/hadoop/hdfs-site.xml</code>, specify the Secondary Node IP, which is by default set to &#39;&#39;&#39;0.0.0.0.0:50090&#39;&#39;&#39; which basically points to all the ip addresses currently configured on the machine and the <code>dfs.replication</code> which is how many copies to store for each block of a file in HDFS (default:3):</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;3&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
        &lt;value&gt;172.16.19.17:50090&lt;/value&gt;
    &lt;/property&gt;
</code></pre></div>
<p>(Recommended) In <code>etc/hadoop/hdfs-site.xml</code>, disable ACL permissions for HDFS files. This might be enabled for a production environment, but for testing, can lead to some Permission denied exceptions:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">    &lt;property&gt;
        &lt;name&gt;dfs.permissions&lt;/name&gt;
        &lt;value&gt;false&lt;/value&gt;
    &lt;/property&gt;
</code></pre></div>
<p><strong>Specific Configuration</strong> 
Now we have an image with the correct hadoop configuration, we proceeded by creating cloned instances from that image to create a small cloud with one name node and 3 data nodes.
The IP address of our machines :</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"> 172.16.19.10 (NameNode)
 172.16.19.11 (DataNode)
 172.16.19.12 (DataNode)
 172.16.19.13 (DataNode)
</code></pre></div>
<p>Now all machines are clones. First we need to adjust the hostnames by doing fixing unknown host.
Then in terms of Hadoop configuration, only the NameNode needs 2 additional configurations :
On the name node server we edited the <code>etc/hadoop/slaves</code> file and added the IP addresses of the 3 data nodes</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"> 172.16.19.11
 172.16.19.12
 172.16.19.13
</code></pre></div>
<p>Now the NameNode knows which slaves it can use. 
Important : Make sure that the NameNode can ssh to each of the DataNodes without a passphrase or any user input. 
 from the NameNode:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"> ssh ubuntu@172.16.19.11
 ssh ubuntu@172.16.19.12
 ssh ubuntu@172.16.19.13
</code></pre></div>
<p>This should be the case if you had already followed the single-node cluster configuration, on one node and then cloned it, as the the ssh key should already be in the list of authorizaed keys for the machine. </p>

<p>On the name node server, edit <code>/etc/hosts</code> add the hostname-ip mappings of the datanodes, so that the file looks like this:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"> ubuntu@namenode:~/hadoop$ cat /etc/hosts
 127.0.0.1 localhost
 127.0.0.1 namenode:localdomain namenode

 &#39;&#39;&#39;172.16.19.11 datanode-1&#39;&#39;&#39;
 &#39;&#39;&#39;172.16.19.12 datanode-2&#39;&#39;&#39;
 &#39;&#39;&#39;172.16.19.13 datanode-3&#39;&#39;&#39;
 # The following lines are desirable for IPv6 capable hosts
 ::1 ip6-localhost ip6-loopback
 fe00::0 ip6-localnet
 ff00::0 ip6-mcastprefix
 ff02::1 ip6-allnodes
 ff02::2 ip6-allrouters
 ff02::3 ip6-allhosts
</code></pre></div>
<p>*On the data nodes, edit <code>/etc/hosts</code> add add the hostname-ip mappings of the  namenode and the other datanodes. This is required because when running Map reduce jobs, the data nodes will need to communicate with each other and to the name node. You also need to (&#39;&#39;&#39;IMPORTANT&#39;&#39;&#39;) comment out the line with localdomain so that the file looks like this:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"> ubuntu@datanode-1:~/hadoop$ cat /etc/hosts
 127.0.0.1 localhost

 #127.0.0.1 datanode-1:localdomain datanode-1

 172.16.19.10 namenode
 172.16.19.11 datanode-1
 172.16.19.12 datanode-2
 172.16.19.13 datanode-3
 ....
</code></pre></div>
<p>Without these modifications, HDFS will work correctly, but running a MapReduce job will fail with a &quot;Connection Refused&quot; error</p>

<p><a name='upload'></a></p>

<h3>Upload Files using HDFS</h3>

<p>Starting the hadoop cluster now by executing the following on NameNode</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">bin/hadoop namenode -format
sbin/start-dfs.sh
sbin/start-yarn.sh
</code></pre></div>
<p>Now we are able to upload files. DFS has its own file namespace, and the stored data on dfs is not directly accessible with Linux commands, instead we have to use <code>bin/hadoop</code> command to access it. By default any relative path on DFS is at <code>/user/[username]/</code> , however this directory is not created by default so we need to create it:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">ubuntu@namenode:~/hadoop$bin/hdfs dfs -mkdir /user
ubuntu@namenode:~/hadoop$bin/hdfs dfs -mkdir /user/ubuntu
</code></pre></div>
<p>You can list the files in /user/ubuntu by :</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">ubuntu@namenode:~/hadoop$bin/hdfs dfs -ls
</code></pre></div>
<p>Note that we didn&#39;t need to specify /user/ubuntu in the ls command, because that&#39;s the default directory.
Now copy a test file and see it with -ls</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">ubuntu@namenode:~/hadoop$echo Hello HDFS &gt; ~/hellohdfs.txt
ubuntu@namenode:~/hadoop$bin/hadoop dfs -put ~/hellohdfs.txt testfile
ubuntu@namenode:~/hadoop$bin/hadoop dfs -ls
Found 1 items
-rw-r--r--   3 ubuntu supergroup         16 2014-11-04 13:06 testfile
</code></pre></div>
<p>We did this while on the namenode, now go to a datanode and see the file content from there:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">ubuntu@datanode-1:~/hadoop$ bin/hadoop dfs -ls
Found 1 items
-rw-r--r--   3 ubuntu supergroup         16 2014-11-04 12:06 testfile
ubuntu@datanode-1:~/hadoop$ bin/hadoop dfs -cat testfile
Hello HDFS
</code></pre></div>
<h3>Using the GUI to browse HDFS</h3>

<p>It is also possible to use the GUI to browse HDFS. For this, have a look at Section <code>Module0: Preparation &amp; Setup - GUI</code></p>

  </article>

</div>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <div class="footer-col-1 column">
      <ul>
        
        
        
        <li>
          <a href="mailto:david.dao@tum.de">david.dao@tum.de</a>
        </li>
      </ul>
    </div>

    <div class="footer-col-2 column">
        
    </div>

    <div class="footer-col-3 column">
      
    </div>

  </div>

</footer>


    <!-- mathjax -->
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </body>
</html>